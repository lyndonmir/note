# 5.1 Memory Technologies
## SRAM
- value is stored on a pair of inverting gates
- fast but more space
## DRAM
- value is stored as a charge on a capacitor(must be refreshed)
- small but slower
## Advaned DRAM Organization
- DRAM accesses an entire row
- Burst mode
- DDR
- QDR
## Flash Storage
## Disk Storage
# 5.2 Memory Hierarchy Introduction
## Locality
- Temporal locality
- Spatial locality
## Taking Advantage of Locality
- store everything on disk
- copy recently accessed(and nearby)items from disks to smaller DRAM memory----Main memory
- copy more recently accessed(and nearby) items from DRAM to SRAM----cache memory attached to cpu
## Some Items
- block
- hit ratio
- miss ratio
- hit time
- miss penalty
# 5.3 The basics of Cache
## Items and Compute
- index
- tag field
- valid bit
### Compute
- Memory block address is larger than cache block address
- 64-bit address(assume)->block
- a direct-mapped cache
- The cache size is $2^{n}$ blocks, so n bits are used for the index
- The block size is $2^{m}$ words($2^{m}\times 2^{2}$) bytes, so m bits are used for the word block, and 2 bits are used for the byte part of the address
- size of the tag field is $$64-(n+m+2)$$
- Total number of bits in a direct-mapped cache is $$2^{n} \times (block\space size+tag\space size+valid\space field\space size)$$ 
## Handling Cache reads hit and Misses
### Data cache miss
### Instruction cache miss
- stall the CPU, fetch block from memory, deliber to cache, restart CPU read
1. Send the original PC value(current PC-4) to the memory
2. Instruct main memory to perform a read and wait for the memory to complete its access
3. Write the cache entry, putting the data from memory in the data portion of the entry, writing the upper bits of the address(from the ALU) into the tag field, and turning the valid bit on
4. Restart the instruction execution at the first step, which will refetch the instruction again, this time finding it in the cache
## Handling Cache Writes hit and Misses
### Write hits
1. write-back: write the data only to the data cache-> write back data from the cache to memory later to accelerate, but cause inconsistent
2. write-through: write the data into both the memory the cache: writes always updata both the cache and the memory->slower--write buffer
### Write misses
- read the entire block into the cache, then write the word
## Q1: Block Placement
### Direct mapped
- Block can only go in one place in the cache, usually address mod number of blocks in cache
### Fully associated
- Block can go anywhere in cache
### Set associated
- Block can go in one of a set of places in the cache
- A set is a group of blocks in the cache-> block address mod number of sets in the cache
- If set have n blocks, the cache is said to be n-way set associative
## Q2: Block Identification
### Tag
- When checking the cache, the processor will compare the requested memory address to the cache tag-- if the two are equal, then there is a cache hit and the data is present in the cache
### Valid bit
### Format
- mentioned before
### The process of addressing
- ![[Orga_Ch5_Risc-v_V1.0.pdf - Adobe Acrobat Pro DC (32-bit) 2023_6_23 23_09_18.png]]
- ![[Orga_Ch5_Risc-v_V1.0.pdf - Adobe Acrobat Pro DC (32-bit) 2023_6_23 23_10_39.png]]
## Q3: Block Replacement
### Strategy of block Replacement
- Random replacement
- Least-recently used(LRU): requires extra bits in the cache to keep track of accesses
- First in,first out(FIFO): the order is match the structure, so no extra bits
## Q4: Write Strategy
### Write-through 
- can always discard cached data- most up-to-data data is in memory
- cache control bit: only a valid bit
- memory always have latest data
### Write-back 
- discard and write back to memory
- valid bit and dirty bit
- much lower brandwidth
### Write stall
- def
- write buffers
### Write misses
- Write allocate: used by write back
- Write around: used by write through
## Performance in Different Memory
### Wider Main Memory
- miss penalty
- bandwidth
### Interleaved Memory
- miss penalty
- bandwidth
## Performance in different block size
- with the increasement of block size, the hit ratio increases
# 5.4 Measuring and improving cache performance
## Formula
- Average Memory Access Time = hit time + miss time = hit rate x Cache time + miss rate x memory time
- CPU time = (CPU execution clock cycles + Memory-stall clock cycles) x Clock cycle time
- Memory-stall clock cycles = \# of instructions x miss rate(instruction miss and data miss) x miss penalty = Read-stall cycles + Write-stall cycles
- Read-stall cycles = Read/Program x Read miss rate x Read miss penalty
- Write-stall cycles = Write/Program x Write miss rate x Write miss penalty
- If the write buffer stalls are small, we can safelty ignore them
- If the cache block size is one word, the write miss penalty is 0
- In most write-through cache organizations, the read nad write miss penalty are the same: the time to fetch block from memory
- If we neglect the write buffer stalls, we get:
- Memory-stall clock cycles = Memory accesses/Program x Miss rate x Miss penalty = Instructions/Program x Misses/Instructions x Miss penalty
- Example
## Reducint cache misses by more flexible placement of blocks
- With Associativity increasing, data miss rate lower
## Locating a  block in the set-associative cache
![[Orga_Ch5_Risc-v_V1.0.pdf - Adobe Acrobat Pro DC (32-bit) 2023_6_24 9_30_26.png]]
## Size of tags versus set associativity
- assume a n-bit address and block size is m, then number of memory block is $2^{n-m}$  
## Choosing which block to replace
- LRU 
- For a two-way set associative cache, the LRU can be implemented easily. We could keep a single bit in each set. We set the bit whenever a specific block in the set is referenced, and reset the bit whenever another block is referenced
- As associativity increases, implementing LRU gets harder
## Decreasing miss penalty with multilevel caches
### Using multilevel caches
- optimize the hit time on the 1st level cache
- optimize the miss rate on the 2nd level cache
# 7.4 Virtual Memory
## Concepts
- Main memory can act as a cache for the secondary storage(disk)
### Pages
- Large number of virtual pages than physical pages
- Page faults: the data is not in the memory, retrieve it from disk
- huge miss penalty, thus pages should be fairly large(4kb)
- reducing page faults is important(LRU)
- can handle faults in software instead of hardware
- using write-through is too expensive so use write back
### Page Tables
- indexed by virtue page number, store into memoryf
- 1 represent in memory, 0 represent in disk
- Page table, program counter and the page table register specifies the state of program
### Page faults
## Reduce size of Page table
- To reduce the actual main memory tied up in page tables, most modern systems also allow the page tables to be paged. Although this sounds tricky, it works by using the same basic ideas of virtual memory and simply allowing the page tables to reside in the virtual address space. In addition, there are some small but critical problems, such as a never-ending series of page faults, which must be avoided. How these problems are overcome is both very detailed and typically highly processor-specific. In brief, these problems are avoided by placing all the page tables in the address space of the operating system and placing at least some of the page tables for the operating system in a portion of main memory that is physically addressed and is always present and thus never in secondary memory.
- Multiple levels of page tables can also be used to reduce the total amount of page table storage, and this is the solution that RISC-V uses to reduce the memory footprint of address translation. Figure 5.29 above shows the four levels of address translation to go from a 48-bit virtual address to a 40-bit physical address of a 4 KiB page. Address translation happens by first looking in the level 0 table, using the highest-order bits of the address. If the address in this table is valid, the next set of high-order bits is used to index the page table indicated by the segment table entry, and so on. Thus, the level 0 table maps the virtual address to a 512 GB (239 bytes) region. The level 1 table in turn maps the virtual address to a 1 GB (230) region. The next level maps this down to a 2 MB (221) region. The final table maps the virtual address to the 4 KiB (212) memory page. This scheme allows the address space to be used in a sparse fashion (multiple noncontiguous segments can be active) without having to allocate the entire page table. Such schemes are particularly useful with very large address spaces and in software systems that require noncontiguous allocation. The primary disadvantage of this multi-level mapping is the more complex process for address translation.
- ![[Pasted image 20230625115154.png]]